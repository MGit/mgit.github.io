# robots.txt - Enhanced Bot Restriction Configuration
# Last updated: June 2025

# Default rule - Block all bots except those explicitly allowed
User-agent: *
Disallow: /

# Allow major search engines for legitimate indexing
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

# Optional: Uncomment if you want to allow other major search engines
# User-agent: Slurp
# Allow: /
# User-agent: DuckDuckBot
# Allow: /

# === AI TRAINING DATA CRAWLERS ===
# Block AI bots that scrape for training data

# OpenAI bots
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: OAI-SearchBot
Disallow: /

# Anthropic (Claude)
User-agent: Claude-Web
Disallow: /

User-agent: ClaudeBot
Disallow: /

# Google AI
User-agent: Google-Extended
Disallow: /

User-agent: Bard
Disallow: /

User-agent: Gemini
Disallow: /

# Microsoft AI
User-agent: ChatGPT
Disallow: /

User-agent: Copilot
Disallow: /

# Meta AI
User-agent: Meta-ExternalAgent
Disallow: /

User-agent: Meta-ExternalFetcher
Disallow: /

# Common Crawl (used by many AI companies)
User-agent: CCBot
Disallow: /

# === SEO TOOLS & SCRAPERS ===
# Block known SEO tools and scrapers

User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: dotbot
Disallow: /

User-agent: Barkrowler
Disallow: /

User-agent: MegaIndex
Disallow: /

User-agent: SiteAuditBot
Disallow: /

User-agent: SEOkicks
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: proximic
Disallow: /

User-agent: Buck
Disallow: /

User-agent: Turnitin
Disallow: /

# === INTERNATIONAL SEARCH ENGINES (Often used for scraping) ===
User-agent: Sogou
Disallow: /

User-agent: Yandex
Disallow: /

User-agent: YandexBot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: 360Spider
Disallow: /

# === SOCIAL MEDIA BOTS ===
User-agent: facebookexternalhit
Disallow: /

User-agent: Twitterbot
Disallow: /

User-agent: LinkedInBot
Disallow: /

User-agent: WhatsApp
Disallow: /

User-agent: TelegramBot
Disallow: /

# === ARCHIVE CRAWLERS ===
User-agent: ia_archiver
Disallow: /

User-agent: archive.org_bot
Disallow: /

User-agent: Wayback
Disallow: /

# === MALICIOUS & AGGRESSIVE CRAWLERS ===
User-agent: SiteSnagger
Disallow: /

User-agent: WebStripper
Disallow: /

User-agent: WebCopier
Disallow: /

User-agent: Offline Explorer
Disallow: /

User-agent: HTTrack
Disallow: /

User-agent: Microsoft URL Control
Disallow: /

User-agent: Xenu Link Sleuth
Disallow: /

User-agent: larbin
Disallow: /

User-agent: libwww
Disallow: /

User-agent: ZmEu
Disallow: /

User-agent: Nikto
Disallow: /

User-agent: sqlmap
Disallow: /

User-agent: w3af
Disallow: /

# === CONTENT SCRAPERS ===
User-agent: Scrapy
Disallow: /

User-agent: Python-urllib
Disallow: /

User-agent: Python-requests
Disallow: /

User-agent: curl
Disallow: /

User-agent: wget
Disallow: /

User-agent: Go-http-client
Disallow: /

User-agent: Node.js
Disallow: /

User-agent: Java
Disallow: /

# === GENERIC PATTERNS TO BLOCK ===
# Block bots that don't respect patterns or have suspicious patterns
User-agent: *
Disallow: /*?*
Disallow: /*.php$
Disallow: /*.json$
Disallow: /*.xml$
Disallow: /*.txt$
Disallow: /api/
Disallow: /private/
Disallow: /admin/
Disallow: /wp-admin/
Disallow: /wp-login.php
Disallow: /wp-content/uploads/
Disallow: /cgi-bin/
Disallow: /search
Disallow: /search/
Disallow: /*sessionid*
Disallow: /*login*
Disallow: /*signup*
Disallow: /*register*
Disallow: /*password*
Disallow: /*auth*
Disallow: /tmp/
Disallow: /temp/
Disallow: /.git/
Disallow: /.svn/
Disallow: /.env
Disallow: /backup/
Disallow: /cache/
Disallow: /logs/

# === RATE LIMITING ===
# Slow down any remaining crawlers
User-agent: *
Crawl-delay: 10

# === SITEMAP ===
# Only provide sitemap to allowed search engines
# Sitemap: https://yourdomain.com/sitemap.xml